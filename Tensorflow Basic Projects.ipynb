{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST dataset with Tensorflow: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "(xtrain, ytrain), (xtest, ytest) = mnist.load_data()\n",
    "print(xtrain.shape, ytrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# We need to flatten xtrain in order to set as input for neural network nd it will be numpy array :\n",
    "# We also need to convert float64 to float32 to minimize computations :\n",
    "# will devide by 255 coz it is grayscale and we need to normalize :\n",
    "xtrain = xtrain.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
    "xtest = xtest.reshape(-1, 28*28).astype(\"float32\") / 255.0\n",
    "print(xtrain.shape, xtest.shape)\n",
    "\n",
    "# This can also done by tensorflow as \n",
    "# xtrain = tf.convert_to_tensor(xtrain)\n",
    "# Similarly for xtest----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 - 8s - loss: 0.2129 - accuracy: 0.9382\n",
      "Epoch 2/5\n",
      "60000/60000 - 3s - loss: 0.0789 - accuracy: 0.9751\n",
      "Epoch 3/5\n",
      "60000/60000 - 3s - loss: 0.0497 - accuracy: 0.9840\n",
      "Epoch 4/5\n",
      "60000/60000 - 4s - loss: 0.0368 - accuracy: 0.9881\n",
      "Epoch 5/5\n",
      "60000/60000 - 4s - loss: 0.0297 - accuracy: 0.9901\n",
      "10000/10000 - 1s - loss: 0.0772 - accuracy: 0.9784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07722423628928372, 0.9784]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequential API (very convenient but not very flexible xoz it map 1 input to 1 output)\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(10),\n",
    "    ]\n",
    ")\n",
    "# IF we do not use Softmax, we can use from_logits=True in loss function that will help to send softmax first :\n",
    "model.compile(\n",
    "    loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    # IF ur Y is hot encoded then use just categorical_crossentropy and if Y is integer USE sparse_categorical_crossentropy\n",
    "    optimizer= keras.optimizers.Adam(lr=0.001),\n",
    "    metrics = [\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(xtrain, ytrain, batch_size=100, epochs=5, verbose=2)\n",
    "model.evaluate(xtest, ytest, batch_size=32, verbose=2)\n",
    "\n",
    "# By adding input above dense as keras.input(shape=(28*28)) , we can get summary model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 - 4s - loss: 0.2388 - accuracy: 0.9319\n",
      "Epoch 2/5\n",
      "60000/60000 - 4s - loss: 0.0889 - accuracy: 0.9724\n",
      "Epoch 3/5\n",
      "60000/60000 - 5s - loss: 0.0574 - accuracy: 0.9821\n",
      "Epoch 4/5\n",
      "60000/60000 - 6s - loss: 0.0418 - accuracy: 0.9869\n",
      "Epoch 5/5\n",
      "60000/60000 - 5s - loss: 0.0297 - accuracy: 0.9906\n",
      "10000/10000 - 1s - loss: 0.0697 - accuracy: 0.9810\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06970541148988996, 0.981]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Functional API (bit more flexible)\n",
    "inputs = keras.Input(shape=(784))\n",
    "x = layers.Dense(512, activation='relu')(inputs)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "outputs = layers.Dense(10, activation='sigmoid')(x)\n",
    "Model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "Model.compile(\n",
    "    loss= keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer= keras.optimizers.Adam(lr=0.001),\n",
    "    metrics = [\"accuracy\"],\n",
    ")\n",
    "\n",
    "Model.fit(xtrain, ytrain, batch_size=100, epochs=5, verbose=2)\n",
    "Model.evaluate(xtest, ytest, batch_size=32, verbose=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR10 Dataset with Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Like before we do not rehape to flatten layer coz we will use convolution model so we will maintain orignal shape height, wid:\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "print(x_train.shape, x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 - 84s - loss: 1.7034 - accuracy: 0.3804\n",
      "Epoch 2/5\n",
      "50000/50000 - 41s - loss: 1.3812 - accuracy: 0.5031\n",
      "Epoch 3/5\n",
      "50000/50000 - 41s - loss: 1.2663 - accuracy: 0.5515\n",
      "Epoch 4/5\n",
      "50000/50000 - 41s - loss: 1.1752 - accuracy: 0.5873\n",
      "Epoch 5/5\n",
      "50000/50000 - 41s - loss: 1.0988 - accuracy: 0.6142\n",
      "10000/10000 - 38s - loss: 1.0657 - accuracy: 0.6260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0656760743141174, 0.626]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will build model : \n",
    "modell = keras.Sequential()\n",
    "modell.add(keras.Input(shape=(32,32,3)))\n",
    "# In layer convnet we will use filter size (3,3) we can write just 3 and also padding VALID OR SAME, default valid.\n",
    "# SAME, they are going to maintain shape, if use VALID, they will change shape WRT kernel size.\n",
    "modell.add(layers.Conv2D(32, 3, padding='valid', activation='relu'))\n",
    "modell.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "modell.add(layers.Conv2D(64, 3, activation='relu'))\n",
    "modell.add(layers.MaxPooling2D())\n",
    "modell.add(layers.Conv2D(128, 3, activation='relu'))\n",
    "modell.add(layers.Flatten())\n",
    "modell.add(layers.Dense(64, activation='relu'))\n",
    "modell.add(layers.Dense(10))\n",
    "\n",
    "modell.compile(\n",
    "    loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer= keras.optimizers.Adam(lr=3e-4),\n",
    "    metrics = [\"accuracy\"],\n",
    ")\n",
    "modell.fit(x_train, y_train, batch_size=100, epochs=5, verbose=2)\n",
    "modell.evaluate(x_test, y_test, batch_size=32, verbose=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model impovement by adding layers (BatchNorm, Dropout, Regularization L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 - 86s - loss: 1.4492 - accuracy: 0.4838\n",
      "Epoch 2/5\n",
      "50000/50000 - 94s - loss: 1.0793 - accuracy: 0.6215\n",
      "Epoch 3/5\n",
      "50000/50000 - 88s - loss: 0.9381 - accuracy: 0.6735\n",
      "Epoch 4/5\n",
      "50000/50000 - 83s - loss: 0.8353 - accuracy: 0.7112\n",
      "Epoch 5/5\n",
      "50000/50000 - 82s - loss: 0.7548 - accuracy: 0.7388\n",
      "10000/10000 - 4s - loss: 1.0731 - accuracy: 0.6307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0731352879524232, 0.6307]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will build model to improve accuracy :\n",
    "def my_model():\n",
    "    inputss = keras.Input(shape=(32,32,3))\n",
    "    x= layers.Conv2D(32, 3, padding='valid')(inputss)\n",
    "    # We saw that we did not use Activation coz we will use batchnorm layer and after that layer we will use Activation:\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, 3,)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    outputss =layers.Dense(10)(x)\n",
    "    modelll = keras.Model(inputs= inputss, outputs=outputss)\n",
    "    return modelll\n",
    "\n",
    "modelll = my_model()\n",
    "\n",
    "modelll.compile(\n",
    "    loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer= keras.optimizers.Adam(lr=3e-4),\n",
    "    metrics = [\"accuracy\"],\n",
    ")\n",
    "modelll.fit(x_train, y_train, batch_size=100, epochs=5, verbose=2)\n",
    "modelll.evaluate(x_test, y_test, batch_size=32, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 - 120s - loss: 3.4640 - accuracy: 0.3352\n",
      "Epoch 2/5\n",
      "50000/50000 - 115s - loss: 2.1887 - accuracy: 0.4847\n",
      "Epoch 3/5\n",
      "50000/50000 - 114s - loss: 1.7346 - accuracy: 0.5514\n",
      "Epoch 4/5\n",
      "50000/50000 - 117s - loss: 1.5262 - accuracy: 0.5865\n",
      "Epoch 5/5\n",
      "50000/50000 - 121s - loss: 1.4005 - accuracy: 0.6044\n",
      "10000/10000 - 4s - loss: 1.3532 - accuracy: 0.6194\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3531849250793457, 0.6194]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we will add Regularizer layer to overcome overfitting nd get better accuracy:\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def myy_model():\n",
    "    inputss = keras.Input(shape=(32,32,3))\n",
    "    x= layers.Conv2D(32, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(inputss)\n",
    "    # We saw that we did not use Activation coz we will use batchnorm layer and after that layer we will use Activation:\n",
    "    # Purpose of batchnormalization is to normalize data\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = layers.Conv2D(64, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = keras.activations.relu(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "    # We will add Dropout here also :\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputss =layers.Dense(10)(x)\n",
    "    modelll = keras.Model(inputs= inputss, outputs=outputss)\n",
    "    return modelll\n",
    "\n",
    "modelll = myy_model()\n",
    "\n",
    "modelll.compile(\n",
    "    loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer= keras.optimizers.Adam(lr=3e-4),\n",
    "    metrics = [\"accuracy\"],\n",
    ")\n",
    "modelll.fit(x_train, y_train, batch_size=100, epochs=5, verbose=2)\n",
    "modelll.evaluate(x_test, y_test, batch_size=32, verbose=2)\n",
    "# We have to increase epoch size coz we have also used Dropout :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Subclassing with Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 - 527s - loss: 2.3019 - accuracy: 0.1111\n",
      "Epoch 2/3\n",
      "60000/60000 - 532s - loss: 2.3013 - accuracy: 0.1124\n",
      "Epoch 3/3\n",
      "60000/60000 - 491s - loss: 2.3013 - accuracy: 0.1124\n",
      "10000/10000 - 14s - loss: 2.3012 - accuracy: 0.1135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3011566482543944, 0.1135]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain = xtrain.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "xtest = xtest.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
    "\n",
    "# We can make own Block instead of using layers multiple times...\n",
    "# Like in previous example we used:    CNN..> ---- BATCH NORM...> ---- Relu....> \n",
    "# X10 \n",
    "class CNNBlock(layers.Layer):\n",
    "    def __init__(self, out_channals, kernel_size = 3):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = layers.Conv2D(out_channals, kernel_size, padding='same')\n",
    "        self.bn = layers.BatchNormalization()\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv(input_tensor)\n",
    "        x = self.bn(x, training=training)\n",
    "        x= tf.nn.relu(x)\n",
    "        return x\n",
    "    \n",
    "model = keras.Sequential(\n",
    "    [ \n",
    "        CNNBlock(32),\n",
    "        CNNBlock(64),\n",
    "        CNNBlock(128),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10),     \n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss= keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer= keras.optimizers.Adam(lr=0.001),\n",
    "    metrics = [\"accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(xtrain, ytrain, batch_size=44, epochs=3, verbose=2)\n",
    "model.evaluate(xtest, ytest, batch_size=32, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\nSomething big custom model like Resnet using CNN Block : \\n\\nclass CNNBlock(layers.Layer):\\n    def __init__(self, out_channals, kernel_size = 3):\\n        super(CNNBlock, self).__init__()\\n        self.conv = layers.Conv2D(out_channals, kernel_size, padding='same')\\n        self.bn = layers.BatchNormalization()\\n    def call(self, input_tensor, training=False):\\n        x = self.conv(input_tensor)\\n        x = self.bn(x, training=training)\\n        x= tf.nn.relu(x)\\n\\nclass ResBlock(layers.layer):\\n    def __init__(self, channels)\\n         super(ResBlock, self).__init__()\\n         self.ccn1 = CNNBlock(channals[0])\\n         self.ccn1 = CNNBlock(channals[1])\\n         self.ccn1 = CNNBlock(channals[2])\\n         self.pooling = layers.Maxpooling2D()\\n         self.identity_mapping = layers.conv2D(channals[1], 3, padding=[3] )\\n           \\n   def call(self, input_tensor, training= False):\\n         x = self.ccn1(input_tensor, training=training)\\n         x = self.ccn1(x, training=training)\\n         x = self.ccn1( x + self.identity_mapping(input_tensor), training=training,)\\n         return self.pooling(x)\\n\\nclass ResNet(keras.Model):\\n     def __init__ (self, num_classes):\\n        super(ResNet, self).__init__()\\n        self.Block1 = ResBlock([32,32,64])\\n        self.Block2 = ResBlock([128,128,256])\\n        self.Block3 = ResBlock([128,256,512])\\n        self.pool = layers.GlobalAveragePooling()\\n        self.classifier = layers.Dense(num_classes)\\n            \\n   def class(self, input_tensor, training = False)\\n          x = self.Block1(input_tensor, training=training)\\n          x = self.Block2(x, training=training)\\n          x = self.Block3(x, training=training)\\n          x = self.pool(x)\\n          return self.classifier(x)\\n         \\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Something big custom model like Resnet using CNN Block : \n",
    "\n",
    "class CNNBlock(layers.Layer):\n",
    "    def __init__(self, out_channals, kernel_size = 3):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv = layers.Conv2D(out_channals, kernel_size, padding='same')\n",
    "        self.bn = layers.BatchNormalization()\n",
    "    def call(self, input_tensor, training=False):\n",
    "        x = self.conv(input_tensor)\n",
    "        x = self.bn(x, training=training)\n",
    "        x= tf.nn.relu(x)\n",
    "\n",
    "class ResBlock(layers.layer):\n",
    "    def __init__(self, channels)\n",
    "         super(ResBlock, self).__init__()\n",
    "         self.ccn1 = CNNBlock(channals[0])\n",
    "         self.ccn1 = CNNBlock(channals[1])\n",
    "         self.ccn1 = CNNBlock(channals[2])\n",
    "         self.pooling = layers.Maxpooling2D()\n",
    "         self.identity_mapping = layers.conv2D(channals[1], 3, padding=[3] )\n",
    "           \n",
    "   def call(self, input_tensor, training= False):\n",
    "         x = self.ccn1(input_tensor, training=training)\n",
    "         x = self.ccn1(x, training=training)\n",
    "         x = self.ccn1( x + self.identity_mapping(input_tensor), training=training,)\n",
    "         return self.pooling(x)\n",
    "\n",
    "class ResNet(keras.Model):\n",
    "     def __init__ (self, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.Block1 = ResBlock([32,32,64])\n",
    "        self.Block2 = ResBlock([128,128,256])\n",
    "        self.Block3 = ResBlock([128,256,512])\n",
    "        self.pool = layers.GlobalAveragePooling()\n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "            \n",
    "   def class(self, input_tensor, training = False)\n",
    "          x = self.Block1(input_tensor, training=training)\n",
    "          x = self.Block2(x, training=training)\n",
    "          x = self.Block3(x, training=training)\n",
    "          x = self.pool(x)\n",
    "          return self.classifier(x)\n",
    "         \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass my_model(keras.Model):\\n      def __init__(self, num_classes = 10):\\n         super(my_model, self).__init__()\\n         self.dense1 = layers.Dense(64)\\n         self.dense2 = layers.Dense(num_classes)\\n    \\n       def call(self, input_tensor):\\n          x = tf.nn.relu(self.dense1(input_tensor))\\n          return self.dense2(x)\\n       \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom Model :\n",
    "\"\"\"\n",
    "class my_model(keras.Model):\n",
    "      def __init__(self, num_classes = 10):\n",
    "         super(my_model, self).__init__()\n",
    "         self.dense1 = layers.Dense(64)\n",
    "         self.dense2 = layers.Dense(num_classes)\n",
    "    \n",
    "       def call(self, input_tensor):\n",
    "          x = tf.nn.relu(self.dense1(input_tensor))\n",
    "          return self.dense2(x)\n",
    "       \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass Dense(layer.layers):\\n      def __init__(self, units, input_dim):\\n          super(Dense, self).__init__()\\n          self.w = self.add_weight(name='w', shape=(input_dim, units), initializer='random_normal', trainable=True)\\n          self.b = self.add_weight(name='b', shape=(units, ), initializer='zeros', trainable=True)\\n          \\n      def call(self, inputs):\\n           return tf.matmul(inputs, self.w) + self.b\\n\\nclass my_model(keras.Model):\\n      def __init__(self, num_classes = 10):\\n         super(my_model, self).__init__()\\n         self.dense1 = Dense(64, 784)\\n         self.dense2 = Dense(10, 64)\\n    \\n       def call(self, input_tensor):\\n          x = tf.nn.relu(self.dense1(input_tensor))\\n          return self.dense2(x)\\n       \\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom Model with custom layers :\n",
    "\"\"\"\n",
    "class Dense(layer.layers):\n",
    "      def __init__(self, units, input_dim):\n",
    "          super(Dense, self).__init__()\n",
    "          self.w = self.add_weight(name='w', shape=(input_dim, units), initializer='random_normal', trainable=True)\n",
    "          self.b = self.add_weight(name='b', shape=(units, ), initializer='zeros', trainable=True)\n",
    "          \n",
    "      def call(self, inputs):\n",
    "           return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "class my_model(keras.Model):\n",
    "      def __init__(self, num_classes = 10):\n",
    "         super(my_model, self).__init__()\n",
    "         self.dense1 = Dense(64, 784)\n",
    "         self.dense2 = Dense(10, 64)\n",
    "    \n",
    "       def call(self, input_tensor):\n",
    "          x = tf.nn.relu(self.dense1(input_tensor))\n",
    "          return self.dense2(x)\n",
    "       \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass MyRelu(layer.layers):\\n      def __init__(self):\\n         super(MyRelu, self).__init__()\\n      \\n      def call(self, x):\\n      return tf.math.maximum(x, 0)\\n\\nclass my_model(keras.Model):\\n      def __init__(self, num_classes = 10):\\n         super(my_model, self).__init__()\\n         self.dense1 = Dense(64)\\n         self.dense2 = Dense(num_classes)\\n    \\n       def call(self, input_tensor):\\n          x = MyRelu(self.dense1(input_tensor))\\n          return self.dense2(x)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom Model regardless of input shapes :\n",
    "\n",
    "\"\"\"\n",
    "class Dense(layer.layers):\n",
    "      def __init__(self, units):\n",
    "          super(Dense, self).__init__()\n",
    "          self.units = units\n",
    "          \n",
    "      def build(self, input_shape)\n",
    "          self.w = self.add_weight(name='w', shape=(input_shape[-1], self.units), initializer='random_normal', trainable=True)\n",
    "          self.b = self.add_weight(name='b', shape=(self.units, ), initializer='zeros', trainable=True)\n",
    "          \n",
    "      def call(self, inputs):\n",
    "           return tf.matmul(inputs, self.w) + self.b\n",
    "           \n",
    "\"\"\"\n",
    "# Making own Relu Class:\n",
    "\"\"\"\n",
    "class MyRelu(layer.layers):\n",
    "      def __init__(self):\n",
    "         super(MyRelu, self).__init__()\n",
    "      \n",
    "      def call(self, x):\n",
    "      return tf.math.maximum(x, 0)\n",
    "\n",
    "class my_model(keras.Model):\n",
    "      def __init__(self, num_classes = 10):\n",
    "         super(my_model, self).__init__()\n",
    "         self.dense1 = Dense(64)\n",
    "         self.dense2 = Dense(num_classes)\n",
    "    \n",
    "       def call(self, input_tensor):\n",
    "          x = MyRelu(self.dense1(input_tensor))\n",
    "          return self.dense2(x)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
